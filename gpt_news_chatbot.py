import os
import pandas as pd
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = st.secrets["OPENAI_API_KEY"]

# GPT ë° ì„ë² ë”© ëª¨ë¸
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3, openai_api_key=api_key)
embedding_model = OpenAIEmbeddings(openai_api_key=api_key)

# ë°ì´í„° ë¡œë”©
excel_path = "newsclip_db_updated.xlsx"
df = pd.read_excel(excel_path)
df.columns = df.columns.str.strip()

# ë²¡í„° DB ì´ˆê¸°í™”
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = []

# ë‰´ìŠ¤ ìš”ì•½ -> Document ë¦¬ìŠ¤íŠ¸ ë³€í™˜
for _, row in df.iterrows():
    if isinstance(row.get("ë‰´ìŠ¤ìš”ì•½"), str) and row["ë‰´ìŠ¤ìš”ì•½"].strip():
        date = str(row.get("ë‚ ì§œ", ""))[:10]
        title = row.get("í•´ë“œë¼ì¸", "")
        summary = row["ë‰´ìŠ¤ìš”ì•½"].strip()
        content = f"[{date}] {title} : {summary}"
        doc = Document(page_content=content, metadata={"title": title, "date": date})
        documents.append(doc)

# ë¬¸ì„œ ë¶„í•  ë° ë²¡í„°í™”
split_docs = text_splitter.split_documents(documents)
vectordb = Chroma.from_documents(split_docs, embedding_model, persist_directory="chromadb")

# Streamlit ì„¤ì •
st.set_page_config(page_title="ë‰´ìŠ¤ ìš”ì•½ GPT ì±—ë´‡", layout="wide")
st.title("ğŸ“° ë‰´ìŠ¤ ìš”ì•½ ê¸°ë°˜ RAG ì§ˆì˜ì‘ë‹µ GPT")

# ì§ˆë¬¸ ì…ë ¥
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: 4ì›” AIDT ê´€ë ¨ ì£¼ìš” ë‰´ìŠ¤ëŠ”?")

# ì§ˆë¬¸ ì²˜ë¦¬
if question:
    with st.spinner("GPTê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
        # RAG: ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        top_docs = vectordb.similarity_search(question, k=5)
        context = "\n".join([doc.page_content for doc in top_docs])

        # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ì•„ë˜ëŠ” ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. íŠ¹íˆ ë¹„ìƒêµìœ¡ ê´€ë ¨ ê¸°ì‚¬ê°€ ìˆë‹¤ë©´ ê°•ì¡°í•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: {question}

        ë‰´ìŠ¤ ìš”ì•½:
        {context}
        """

        try:
            response = llm.predict(prompt)
            st.markdown(
                f"""
                <div style='background-color: #f9f9f9; padding: 20px; border-radius: 12px;'>
                    <h4 style='color:#333;'>ğŸ’¬ ì§ˆë¬¸</h4>
                    <p style='font-size:16px;'>{question}</p>
                    <hr style="margin-top: 20px; margin-bottom: 20px;">
                    <h4 style='color:#333;'>ğŸ“˜ ë‹µë³€</h4>
                    <p style='font-size:16px; line-height:1.6;'>{response}</p>
                </div>
                """,
                unsafe_allow_html=True
            )
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
